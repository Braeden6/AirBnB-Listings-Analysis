---
title: "AirBnB Listings: An in depth dive into the world of short-term sublets"
author: 'Armandas Bartas, Alex Romanus, Braeden Norman, Gabriel Lanzaro'
date: "`r Sys.Date()`"
output: 
  bookdown::pdf_document2:
    includes:
      in_header: preamble.tex
    number_sections: false
    toc: FALSE
---

```{r setup, warning=FALSE, message=FALSE, echo=FALSE}
library(tidyverse)
library(testthat)
listings <- read.csv("data/Listings_updated.csv")
amenitiesCount <- read.csv("data/amenities_count.csv")
require(gridExtra)
library(dplyr)
library(janitor)
library(png)
```

\maketitle
\section{Introduction}

This project aims to investigate  AirBnb listings and obtain insights into the most important features of short-term sublets. More specifically, the goal of this project is to classify the cities based on different attributes. The dataset, which was obtained from Kaggle, contains 10 cities from very distinct parts of the world: Bangkok, Cape Town, Hong Kong, Istambul, Mexico City, New York, Paris, Rio de Janeiro, Rome, and Sydney. The Airbnb data possesses 280 000 listings including information related to host info, geographical data, price, number of bedrooms, amenities, review scores, etc.

Previous research has shown that online review scores significantly contribute to selecting a place to stay (Zhao et al., 2015; Thomsen and Jeong, 2020). Moreover, the availability of locations with great review scores can influence the choice of a destination to spend a vacation. Therefore, it is crucial to develop models to further understand the destination selection. Local travel agencies can then target specific factors for improvement (e.g., reducing prices, setting mandatory amenities, demanding minimum review scores to keep hosts in the system).

The analysis can then reveal important aspects regarding how different attributes may characterize each city, for example: \newline
- Which amenities are more important for each city when selecting a property?\newline
- Does the host profile differ among different cities?\newline
- Which types of accommodation are more common depending on the city?\newline
- Can we predict the city based on different preferences related to the place to stay? \newline
- What are the most important AirBnB variables to predict a city for destination?

This project uses two classification algorithms to predict the cities: random forests and standard multinomial classification techniques. These methods have been selected as they provide different inferences about the data.

\maketitle
\section{Exploratory Analysis}

The exploratory analysis revealed some important information about the dataset. For example, the next figure shows graphs that present (1) the number of guests the listing accommodates, (2) for how long the host has been renting properties in AirBnB, and (3) the room types for different categories. The first boxplot indicates that cities such as Cape Town, Rio de Janeiro, and Rome tend to offer listings with more guests, which might be suitable for group or family trips. The second boxplot shows that AirBnb has been used in some cities for more time than in others. For example, New York and Paris have, on average, hosts with more AirBnb time than in Istambul. It might show that AirBnb has been widely used in Istambul only recently. The third plot shows that most of the accommodations in Paris and Cape Town are for the entire place, and most of the accommodations in Hong Kong are for private rooms.

```{r accomodates-and-days, echo=FALSE, warning=FALSE, message=FALSE, out.width="100%"}

p1 <- ggplot(listings, aes(city, accommodates)) + geom_boxplot() +
  xlab('City') + ylab('Accomodates') +
  theme(axis.text.x = element_text(angle = 45, vjust = 1, hjust = 1))

time_differences <- difftime(Sys.Date(),as.Date(listings$host_since), units = c("days"))
listings$time_diff <- as.numeric(time_differences)

p2 <- ggplot(listings, aes(city, time_differences)) + geom_boxplot() +
  xlab('City') + ylab('Days as a Host') +
  theme(axis.text.x = element_text(angle = 45, vjust = 1, hjust = 1))

p3 <- ggplot(listings, aes(x = city, fill = room_type)) + 
  geom_bar(position = "fill") +
  labs(y = "Proportion of Room Type", fill = "Room Type", x = "City") +
  theme(axis.text.x = element_text(angle = 45, vjust = 1, hjust = 1), legend.position="bottom", legend.direction="vertical", legend.key.size = unit(0.2, "cm"))

grid.arrange(p1, p2, p3, ncol=3)

```

In the original dataset, there is a column with a list of the possible amenities a listing has. There were `r nrow(amenitiesCount)` different amenities and a lot of them only have a count of 1. So, to better view the distribution, we removed all amenities that had below 100 observations. The plots below represent the percent TRUE/FALSE values of 2 of the 60 included amenities in the updated dataset. For example, in Heating, we only have 4 cities that have almost no listings with heating (i.e., Bangkok, Hong Kong, Mexico City, and Rio de Janeiro). These cities tend to have the highest temperatures throughout the year.

```{r exploring-data, out.width="50%", echo=FALSE, fig.height= 4}
reducedAmenities <- amenitiesCount$V1[which(amenitiesCount$V2 > 10000)]
selected <- c(1,6)
knitr::opts_chunk$set(fig.width=unit(18,"cm"), fig.height=unit(5,"cm"))
for (ii in selected) {
   plt <- ggplot(listings, aes(x = city, 
      fill = factor(listings[,str_replace_all(reducedAmenities[ii], " ", ".")],
      levels = c(0, 1), labels = c("False", "True")))) +
      geom_bar(position = "fill") +
      labs(y = "Percent", fill = "", x = "City", 
      title =  paste(reducedAmenities[ii],"by City")) +
    theme(axis.text.x = element_text(angle = 90, vjust = 0.5, hjust=1))
    print(plt)
}

```

The ratings were also evaluated. The high medians of Cape Town and Rio suggest that these cities may be easier to predict than the rest. Hong Kong also seems to have a significantly lower rating spread of ratings than the rest of the cities. Generally, it seems Airbnb customers seem to give high reviews.

```{r exploring-data-review-scores-rating, message=FALSE, warning=FALSE, echo=FALSE}
df <- data.frame(listings)

df <- (df %>% group_by(city) %>% mutate(price_standardized = price/max(price)))

df_no_NAN <- df[!is.na(df$review_scores_rating),]

ggplot(df_no_NAN, aes(city, review_scores_rating)) + geom_boxplot() +
xlab('City') + ylab('Review Scores of Listings Overall Ratings') +
theme(axis.text.x = element_text(angle = 45, vjust = 1, hjust = 1))

```

The following table shows the proportion of reviews. The largest proportion of high reviews, i.e., 9s and 10s, belong to Paris at 24.99% and 26.09% respectively. Also, Istanbul has large shares of low reviews, with 23.15%, 28.57%, and 22.22% of 2s, 3s, and 4s, respectively, compared to the other cities. These heavy tails may make it easier to classify these two cities if review scores based on location are a significant predictor in the model.

```{r exploring-data-review-scores-location, message=FALSE, warning=FALSE, echo=FALSE}
df_no_NAN <- df[!is.na(df$review_scores_location),]


table1 <- tabyl(df_no_NAN, city, review_scores_location)

table2 <- tabyl(df_no_NAN, city, review_scores_location)%>%
adorn_percentages("col") %>%
adorn_pct_formatting(digits = 2)

knitr::kable(table2)
```

\maketitle
\section{Results and Analysis}

\subsection{Standard Classification Techniques}

This report considered classification models to determine the different cities based on the AirBnb data. Seven different classification models were applied to the dataset: (1) lasso logistic regression with default lambda, (2) lasso logistic regression with the lambda selected via cross-validation, (3) ridge logistic regression with default lambda, (4) ridge logistic regression with lambda selected via cross-validation, (5) linear discriminant analysis (LDA), (6) quadratic discriminant analysis (QDA), and (7) logistic regression. 

The dataset was split into two sets: a training dataset with 80% of the data and a test dataset with 20% of the data. The errors were then computed using the test dataset. Out of the seven proposed models, the lasso logistic regression with lambda selected via cross-validation presented the best performance because of its lower error (i.e., higher accuracy).

```{r model-error-results, message=FALSE, warning=FALSE, echo=FALSE, fig.width = 7}
load("data/modelComparison.RData")

knitr::kable(results, col.names = c('CV Lasso', 'Default Lasso', 'CV Ridge', 'Default Ridge', 'LDA', 'QDA', 'Logistic Regression'))
```

The following plots show different values of lambda for the lasso and the ridge logistic regressions. It also shows the number of predictors that were used in each model. Lasso performs better because it uses a subset of the predictors, and some of the predictors were not useful in the final model. Therefore, the accuracy of the lasso  model with fewer predictors outperformed the ridge model.

```{r model-comparison-lasso-ridge, message=FALSE, warning=FALSE, echo=FALSE, fig.width = 7}
lasso <- readPNG("plot/glmnet_cv_fit.png")
grid::grid.raster(lasso)
```

Preprocessing was also used to try to increase the model performance. The following tables shows the accuracies with different preprocessing techniques. By default, glmnet standardizes the data. Therefore, no differences were found between the accuracy reported previously and the accuracy with standardization. In addition, there were no great differences by implementing the min-max normalization.

```{r model-error-results_normalize, message=FALSE, warning=FALSE, echo=FALSE}
knitr::kable(results_with_normalization, col.names = c('CV Lasso', 'CV Lasso with Standardization', 'CV Lasso with Min-Max Normalization'))
```

The following tables show the accuracy of the model by city. The first table is a confusion matrix, and the second is the accuracy by city. Rio de Janeiro had the highest accuracy of all cities: it presented an accuracy of approximately 83%.

```{r confusion-matrix, message=FALSE, warning=FALSE, echo=FALSE}
knitr::kable(confusionMat$table, col.names = c('BNK', 'CPT', 'HKG', 'IST', 'MXC', 'NYC', 'PAR', 'RIO', 'ROM', 'SYD'))

knitr::kable(cityAccuracy, col.names = c('City', 'Accuracy'), digits = 3)
```

Finally, the plots below display the coefficient values for the different cities. The figures show that the coefficients may vary considerably depending on the city. For example, the coefficients for Sydney are very different from the coefficients for Cape Town. It shows the variables may exert different degrees of influence in different cities.

```{r model-results_norm, message=FALSE, warning=FALSE, echo=FALSE, fig.width = 7}
norm <- readPNG("plot/l1_norm_lasso.png")
grid::grid.raster(norm)
```

\subsection{Random Forest Analysis}

The most important tuning parameter with regards to random forest is mtry - the number of randomly selected predictors to use when building each individual tree. Greedy descent was performed on mtry, starting at mtry = 1, and adding +1 at each iteration until it no longer improved the out of bag error. This analysis, and the following on nodesize (i.e., the minimum size of terminal nodes), were done using a 10 000 row subset of the original data set in order to decrease computation time. Models with an mtry value of 7 performed slightly better than those with the default parameter of 9. No analysis was done on higher values of mtry.  

```{r, echo = FALSE, fig.width = 6, fig.height=3}
mtry_vals = 1:8
mtry_oob_err = c(37.29, 20.61, 16.94, 15.48, 14.98, 14.83, 14.48, 14.51)

ggplot(as.data.frame(cbind(mtry_vals, mtry_oob_err)), aes(x = mtry_vals, y = mtry_oob_err)) +
  geom_line(color = "red") +
  geom_point() +
  xlab("Mtry Value") +
  ylab("Out-of-Bag Error, %") +
  ggtitle("Greedy Descent on Mtry")
```
  
A greedy descent was also done on the maxnodes parameter - the maximum number of terminal nodes in each tree. However, larger values or maxnodes took too long to compute and the analysis was abandoned. Nodesize - the minimum size of terminal nodes, was found to be a better tuning parameter to modify. Trying different values for nodesize revealed that the default value for classification, 1, gave the best out-of-bag error.  

```{r, echo = FALSE, fig.width=6, fig.height = 3}

nodesize_vals = 1:11
nodesize_oob_err = c(23.7, 26.0, 24.4, 24.4, 26.1, 25.1, 25.8, 25.1, 25.3, 26.3, 26.7)

ggplot(as.data.frame(cbind(nodesize_vals, nodesize_oob_err)), aes(x = nodesize_vals, y = nodesize_oob_err)) +
  geom_line(color = "red") +
  geom_point() +
  xlab("Nodesize Value") +
  ylab("Out-of-Bag Error, %") +
  ggtitle("Nodesize Value verus Error")
```
  
Class weights were added in an attempt to improve the model. The hypothesis was that weighing the harder-to-predict classes more heavily would improve the classification error. However, it did occur. For completeness, those calsses were weighted more lightly compared to the easier-to-predict classes. This  did not have any substantial effect on the error either.  
  
Variable selection was evaluated using the rfcv() function with a reduced dataset. It was found that eliminating variables increased the out-of-bag error. It is worth noting that these reduced models would theoretically have lower variance but increased bias. It is also worth noting that the increase in error resulting from variable reduction was larger when fitted on a larger dataset. The reduced model with 42 predictor variables and the full model with 83 variables were then fitted on the full dataset (complete cases only). 
``` {r, echo = FALSE, fig.width = 6, fig.height = 3}
# testcv = readRDS("testcv.rds")
# testcv2 = readRDS("testcv2.rds")
# cvdf <- as.data.frame(cbind(testcv$error.cv, testcv2$error.cv))
# ggplot(cvdf, aes(as.numeric(rownames(cvdf)))) +
#   geom_point(aes(y = V2)) +
#   geom_line(aes(y = V2, col = "N = 10 000")) +
#   xlab("Number of Variables in Model") +
#   ylab("CV Error") +
#   geom_point(aes(y = V1)) +
#   geom_line(aes(y = V1, col = "N = 1 000"))

cvrfplot <- readPNG("plot/cvdfplot.png")
grid::grid.raster(cvrfplot)
```

\maketitle
\section{Discussions}

```{r, echo = FALSE}
# fullrf <- readRDS(file = "fullrf.rds")
# reducedrf <- readRDS(file = "reducedrf.rds")
# fullrf
# reducedrf
# FILES ARE TOO LARGE TO UPLOAD TO GITHUB (:
```

This report used different classification techniques to predict the cities based on an Airbnb dataset. 10 different cities from very distinct parts of the world were used. The explanatory analysis revealed that many variables (e.g., amenities, review scores, number of days as a host) influenced the cities. These variables were used to fit the classification models using traditional multinomial logistic models and random forest models.

After all the analysis, two random forest models were fitted with mtry set to 7. The first model was fitted over all variables and gave an out-of-bag classification error of 10.01%. The second model was fitted over the 42 variables marked most important by the random forest algorithm in the first model. It achieved an out-of-bag classification error of 11.03%. The easiest class to classify was Bangkok. The most difficult were Hong Kong and Istanbul, which often were mistaken for each-other. Sidney was also misclassified often as either New York or Paris. Overall, the random forest model performed very well and with minimal adjustments from the default parameters.

By comparing the random forest results to the traditional classification techniques (e.g., logistic regression), it can be seen that random forests presented better performance. In addition, random forests can provide additional and clearer inferences using the variance importance plot. Future works can include deep learning techniques as an attempt to further improve the performance of the model.

\maketitle
\section{References}

Thomsen, Chuhan Renee, and Miyoung Jeong. "An analysis of Airbnb online reviews: user experience in 16 US cities." Journal of Hospitality and Tourism Technology (2020).

Zhao, Xinyuan Roy, et al. "The influence of online reviews to online hotel booking intentions." International Journal of Contemporary Hospitality Management (2015).
